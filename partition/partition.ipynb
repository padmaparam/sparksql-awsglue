{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6174ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34482889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[8] appName=PySpark Partition>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Scripts instantiates a SparkSession locally with 8 worker threads.\n",
    "'''\n",
    "appName = \"PySpark Partition\"\n",
    "master = \"local[8]\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "# ref: https://towardsai.net/p/programming/pyspark-aws-s3-read-write-operations\n",
    "#spark configuration\n",
    "conf = SparkConf().set('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " setAppName(appName).setMaster(master)\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "# read aws credentials\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(r'C:\\Users\\padma\\.aws\\credentials'))\n",
    "\n",
    "accessKeyId= config['default']['AWS_ACCESS_KEY_ID']\n",
    "secretAccessKey= config['default']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', accessKeyId)\n",
    "hadoopConf.set('fs.s3a.secret.key', secretAccessKey)\n",
    "hadoopConf.set('fs.s3a.endpoint', 's3.amazonaws.com')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "print(sc)\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f318907",
   "metadata": {},
   "source": [
    "## Data partitioning \n",
    "Data partitioning is critical to data processing performance especially for large volume of data processing in Spark. \n",
    "Partitions in Spark won’t span across nodes though one node can contains more than one partitions. When processing,\n",
    "Spark assigns one task for each partition and each worker threads can only process one task at a time. Thus, \n",
    "with too few partitions, the application won’t utilize all the cores available in the cluster and it can cause \n",
    "data skewing problem; with too many partitions, it will bring overhead for Spark to manage too many small tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b739345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n",
      "+-------+----------+------+\n",
      "|Country|      Date|Amount|\n",
      "+-------+----------+------+\n",
      "|     CN|2019-01-01|    10|\n",
      "|     AU|2019-01-01|    10|\n",
      "|     CN|2019-01-02|    11|\n",
      "|     AU|2019-01-02|    11|\n",
      "|     CN|2019-01-03|    12|\n",
      "|     AU|2019-01-03|    12|\n",
      "|     CN|2019-01-04|    13|\n",
      "|     AU|2019-01-04|    13|\n",
      "|     CN|2019-01-05|    14|\n",
      "|     AU|2019-01-05|    14|\n",
      "|     CN|2019-01-06|    15|\n",
      "|     AU|2019-01-06|    15|\n",
      "|     CN|2019-01-07|    16|\n",
      "|     AU|2019-01-07|    16|\n",
      "|     CN|2019-01-08|    17|\n",
      "|     AU|2019-01-08|    17|\n",
      "|     CN|2019-01-09|    18|\n",
      "|     AU|2019-01-09|    18|\n",
      "|     CN|2019-01-10|    19|\n",
      "|     AU|2019-01-10|    19|\n",
      "+-------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Scripts to populate a data frame with 100 records.\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
    "\n",
    "print(spark.version)\n",
    "# Populate sample data\n",
    "start_date = date(2019, 1, 1)\n",
    "data = []\n",
    "for i in range(0, 50):\n",
    "    data.append({\"Country\": \"CN\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})\n",
    "    data.append({\"Country\": \"AU\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})\n",
    "\n",
    "schema = StructType([StructField('Country', StringType(), nullable=False),\n",
    "                     StructField('Date', DateType(), nullable=False),\n",
    "                     StructField('Amount', IntegerType(), nullable=False)])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd0f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data frame to file system\n",
    "# 8 sharded files will be generated for each partition under folder data/example.csv\n",
    "# 7 shards/files with 12 rows and one file with 16 rows\n",
    "df.count()\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08b8ac",
   "metadata": {},
   "source": [
    "## Repartitioning with coalesce function\n",
    "This function is defined as the following:\n",
    "<pre>\n",
    "def coalesce(numPartitions)\n",
    "Returns a new :class:DataFrame that has exactly numPartitions partitions.\n",
    "</pre>\n",
    "\n",
    "This operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "See below:\n",
    "Now if we run the following code, can you guess how many sharded files will be generated?\n",
    "The answer is still 8. **This is because coalesce function does’t involve reshuffle of data.** \n",
    "In the above code, we want to increate the partitions to 16 but the number of partitions \n",
    "stays at the current (8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e619ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "df = df.coalesce(16)\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386bb76",
   "metadata": {},
   "source": [
    "If we decrease the partitions to 4 by running the following code, how many files will be generated? The answer is 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f57f690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "df = df.coalesce(4)\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff3b64",
   "metadata": {},
   "source": [
    "## Repartitioning with repartition function\n",
    "The other method for repartitioning is repartition. It’s defined as the follows:\n",
    "<pre>\n",
    "def repartition(numPartitions, *cols)\n",
    "</pre>\n",
    "Returns a new :class:DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
    "\n",
    "numPartitions can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
    "\n",
    "Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
    "optional if partitioning columns are specified.\n",
    "\n",
    "Data reshuffle occurs when using this function. Let’s try some examples using the above dataset.\n",
    "\n",
    "### Repartition by number\n",
    "Use the code below to repartition the data to 10 partitions.\n",
    "Spark will try to evenly distribute the data to each partitions. If the total partition number is greater than the actual record count (or RDD size), some partitions will be empty. After we run the above code, data will be reshuffled to 10 partitions with 10 sharded files generated.\n",
    "\n",
    "If we repartition the data frame to 1000 partitions, how many sharded files will be generated?\n",
    "The answer is 100 because the other 900 partitions are empty and each file has one record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5a1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df = df.repartition(10)\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf4969",
   "metadata": {},
   "source": [
    "### Repartition by column\n",
    "We can also repartition by columns.\n",
    "For example, let’s run the code below to repartition the data by column Country.\n",
    "This will create 200 partitions (**Spark by default create 200 partitions**).  However only three sharded files are generated:\n",
    "- One file stores data for CN country.\n",
    "- Another file stores data for AU country.\n",
    "- The other one is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ac5aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "df = df.repartition(\"Country\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5750e",
   "metadata": {},
   "source": [
    "Similarly, if we can also partition the data by Date column:\n",
    "<pre>\n",
    "df = df.repartition(\"Date\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)\n",
    "</pre>\n",
    "If you look into the data, you may find the data is probably not partitioned properly as you would expect, for example, one partition file only includes data for both countries and different dates too.\n",
    "\n",
    "**This is because by default Spark use hash partitioning as partition function**. You can use range partitioning function or customize the partition functions. I will talk more about this in my other posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37f8df",
   "metadata": {},
   "source": [
    "### Partition by multiple columns\n",
    "In real world, you would probably partition your data by multiple columns. To implement the multiple column partitioning strategy, we need to derive some new columns (year, month, date). Code below derives some new columns and then repartition the data frame with those columns.\n",
    "\n",
    "When you look into the saved files, you may find that all the new columns are also saved and the files still mix different sub partitions. To improve this, we need to match our write partition keys with repartition keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a97e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+-----+---+\n",
      "|Country|      Date|Amount|Year|Month|Day|\n",
      "+-------+----------+------+----+-----+---+\n",
      "|     AU|2019-01-21|    30|2019|    1| 21|\n",
      "|     CN|2019-01-29|    38|2019|    1| 29|\n",
      "|     AU|2019-01-19|    28|2019|    1| 19|\n",
      "|     AU|2019-02-02|    42|2019|    2|  2|\n",
      "|     AU|2019-02-07|    47|2019|    2|  7|\n",
      "|     AU|2019-02-05|    45|2019|    2|  5|\n",
      "|     AU|2019-02-08|    48|2019|    2|  8|\n",
      "|     CN|2019-01-27|    36|2019|    1| 27|\n",
      "|     CN|2019-01-21|    30|2019|    1| 21|\n",
      "|     CN|2019-01-25|    34|2019|    1| 25|\n",
      "|     CN|2019-02-06|    46|2019|    2|  6|\n",
      "|     AU|2019-01-11|    20|2019|    1| 11|\n",
      "|     CN|2019-01-19|    28|2019|    1| 19|\n",
      "|     CN|2019-02-19|    59|2019|    2| 19|\n",
      "|     AU|2019-02-03|    43|2019|    2|  3|\n",
      "|     AU|2019-02-09|    49|2019|    2|  9|\n",
      "|     CN|2019-01-14|    23|2019|    1| 14|\n",
      "|     AU|2019-01-16|    25|2019|    1| 16|\n",
      "|     CN|2019-02-16|    56|2019|    2| 16|\n",
      "|     AU|2019-01-10|    19|2019|    1| 10|\n",
      "+-------+----------+------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# derive some new columns (year, month, date)\n",
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\n",
    "\"Month\", month(\"Date\")).withColumn(\"Day\", dayofmonth(\"Date\"))\n",
    "# repartition the data frame with new columns\n",
    "df = df.repartition(\"Year\", \"Month\", \"Day\", \"Country\")\n",
    "df.show()\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80811982",
   "metadata": {},
   "source": [
    "### partitionBy\n",
    "When you look into the saved files, you may find that all the new columns are also saved and the files still mix different sub partitions. To improve this, we need to match our write partition keys with repartition keys.\n",
    "To match partition keys, we just need to change the last line to add a partitionBy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e99fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+-----+---+\n",
      "|Country|      Date|Amount|Year|Month|Day|\n",
      "+-------+----------+------+----+-----+---+\n",
      "|     AU|2019-01-21|    30|2019|    1| 21|\n",
      "|     CN|2019-01-29|    38|2019|    1| 29|\n",
      "|     AU|2019-01-19|    28|2019|    1| 19|\n",
      "|     AU|2019-02-02|    42|2019|    2|  2|\n",
      "|     AU|2019-02-07|    47|2019|    2|  7|\n",
      "|     AU|2019-02-05|    45|2019|    2|  5|\n",
      "|     AU|2019-02-08|    48|2019|    2|  8|\n",
      "|     CN|2019-01-27|    36|2019|    1| 27|\n",
      "|     CN|2019-01-21|    30|2019|    1| 21|\n",
      "|     CN|2019-01-25|    34|2019|    1| 25|\n",
      "|     CN|2019-02-06|    46|2019|    2|  6|\n",
      "|     AU|2019-01-11|    20|2019|    1| 11|\n",
      "|     CN|2019-01-19|    28|2019|    1| 19|\n",
      "|     CN|2019-02-19|    59|2019|    2| 19|\n",
      "|     AU|2019-02-03|    43|2019|    2|  3|\n",
      "|     AU|2019-02-09|    49|2019|    2|  9|\n",
      "|     CN|2019-01-14|    23|2019|    1| 14|\n",
      "|     AU|2019-01-16|    25|2019|    1| 16|\n",
      "|     CN|2019-02-16|    56|2019|    2| 16|\n",
      "|     AU|2019-01-10|    19|2019|    1| 10|\n",
      "+-------+----------+------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# derive some new columns (year, month, date)\n",
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\n",
    "\"Month\", month(\"Date\")).withColumn(\"Day\", dayofmonth(\"Date\"))\n",
    "# repartition the data frame with new columns\n",
    "df = df.repartition(\"Year\", \"Month\", \"Day\", \"Country\")\n",
    "df.show()\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.partitionBy(\"Year\", \"Month\", \"Day\", \"Country\").mode(\n",
    "\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
