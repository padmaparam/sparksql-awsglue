{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6174ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable pyspark, uncomment and run the following only for windows.\n",
    "# import findspark\n",
    "# findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f91030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/praghavan/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229faf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent process: psutil.Process(pid=48799, name='python3.9', status='running', started='11:39:23')\n",
      "No child threads associated\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "current_process = psutil.Process()\n",
    "print(f'Parent process: {current_process}')\n",
    "children = current_process.children(recursive=True)\n",
    "count_child_threads = 0\n",
    "for child in children:\n",
    "    print('Child pid is {}'.format(child.pid))\n",
    "    count_child_threads +=1\n",
    "if count_child_threads == 0:\n",
    "    print('No child threads associated')\n",
    "else:\n",
    "    print(f'{count_child_threads} child threads associated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scripts instantiates a SparkSession locally with 8 worker threads.\n",
    "'''\n",
    "appName = \"PySpark Partition\"\n",
    "# here 8  refers to 8 threads/tasks/executors\n",
    "master = \"local[8]\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "# ref: https://towardsai.net/p/programming/pyspark-aws-s3-read-write-operations\n",
    "#spark configuration\n",
    "conf = SparkConf().set('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " setAppName(appName).setMaster(master)\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "# read aws credentials\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(r'/Users/praghavan/.aws/credentials'))\n",
    "\n",
    "accessKeyId= config['default']['AWS_ACCESS_KEY_ID']\n",
    "secretAccessKey= config['default']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', accessKeyId)\n",
    "hadoopConf.set('fs.s3a.secret.key', secretAccessKey)\n",
    "hadoopConf.set('fs.s3a.endpoint', 's3.amazonaws.com')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "print(sc)\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803b5ab",
   "metadata": {},
   "source": [
    "## Data partitioning \n",
    "Data partitioning is critical to data processing performance especially for large volume of data processing in Spark. \n",
    "Partitions in Spark won’t span across nodes though one node can contains more than one partitions. When processing,\n",
    "Spark assigns one task for each partition and each worker threads can only process one task at a time. Thus, \n",
    "with too few partitions, the application won’t utilize all the cores available in the cluster and it can cause \n",
    "data skewing problem; with too many partitions, it will bring overhead for Spark to manage too many small tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b739345",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lt/m2xlrp8d15757pk_gxyvnpzh0000gq/T/ipykernel_48799/1091286751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDateType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Populate sample data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Scripts to populate a data frame with 100 records.\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
    "\n",
    "print(spark.version)\n",
    "# Populate sample data\n",
    "start_date = date(2019, 1, 1)\n",
    "data = []\n",
    "for i in range(0, 50):\n",
    "    data.append({\"Country\": \"CN\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})\n",
    "    data.append({\"Country\": \"AU\", \"Date\": start_date +\n",
    "                 timedelta(days=i), \"Amount\": 10+i})\n",
    "\n",
    "schema = StructType([StructField('Country', StringType(), nullable=False),\n",
    "                     StructField('Date', DateType(), nullable=False),\n",
    "                     StructField('Amount', IntegerType(), nullable=False)])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read column type and apply null to the column based on column type - string or int, etc\n",
    "'''\n",
    "from pyspark.sql import SparkSession, Column, functions as F\n",
    "cols = df.columns\n",
    "for field in cols:\n",
    "    original_col_type = df.select(field).dtypes[0][1]\n",
    "    print(original_col_type)\n",
    "    df_exp = df.withColumn(field, F.lit(None).cast(original_col_type))\n",
    "    df_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48bf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Column, functions as F\n",
    "\n",
    "#df1 =df.withColumn('Country', F.lit(None).cast('string'))\n",
    "#df1 =df.withColumn('Country1', column)\n",
    "\n",
    "\n",
    "'''\n",
    "Following replaces the  column name value CN with Canada\n",
    "'''\n",
    "#column:Column = F.when(F.col(\"Country\")=='CN', 'CANADA').otherwise(F.col(\"Country\"))\n",
    "## column:Column = F.when(F.col(\"Country\")=='CN', 'CANADA')\n",
    "## df1 =df.withColumn('Country', column)\n",
    "#df1 =df.withColumn('Country1', column)\n",
    "\n",
    "# # filter out rows where Country is CN'\n",
    "column:Column = F.when(F.col(\"Country\")=='CN', True).otherwise(False)\n",
    "df_CN = df.filter(column)\n",
    "# # filter out rows where Country is not CN'\n",
    "df_none_CN = df.filter(~column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901903cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CN.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_none_CN.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d428ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data frame to file system\n",
    "# 8 sharded files will be generated for each partition under folder data/example.csv\n",
    "# 7 shards/files with 12 rows and one file with 16 rows\n",
    "df.count()\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807a961",
   "metadata": {},
   "source": [
    "## Repartitioning with coalesce function\n",
    "This function is defined as the following:\n",
    "<pre>\n",
    "def coalesce(numPartitions)\n",
    "Returns a new :class:DataFrame that has exactly numPartitions partitions.\n",
    "</pre>\n",
    "\n",
    "This operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "See below:\n",
    "Now if we run the following code, can you guess how many sharded files will be generated?\n",
    "The answer is still 8. **This is because coalesce function does’t involve reshuffle of data.** \n",
    "In the code below, we want to increase the partitions to 16 but the number of partitions\n",
    "stays at the current (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96882e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.coalesce(16)\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3843bb",
   "metadata": {},
   "source": [
    "If we decrease the partitions to 4 by running the following code, how many files will be generated? The answer is 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67526b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.coalesce(4)\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b2201",
   "metadata": {},
   "source": [
    "## Repartitioning with repartition function\n",
    "The other method for repartitioning is repartition. It’s defined as the follows:\n",
    "<pre>\n",
    "def repartition(numPartitions, *cols)\n",
    "</pre>\n",
    "Returns a new :class:DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
    "\n",
    "numPartitions can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
    "\n",
    "Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
    "optional if partitioning columns are specified.\n",
    "\n",
    "Data reshuffle occurs when using this function. Let’s try some examples using the above dataset.\n",
    "\n",
    "### Repartition by number\n",
    "Use the code below to repartition the data to 10 partitions.\n",
    "Spark will try to evenly distribute the data to each partitions. If the total partition number is greater than the actual record count (or RDD size), some partitions will be empty. After we run the above code, data will be reshuffled to 10 partitions with 10 sharded files generated.\n",
    "\n",
    "If we repartition the data frame to 1000 partitions, how many sharded files will be generated?\n",
    "The answer is 100 because the other 900 partitions are empty and each file has one record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1000)\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91a579",
   "metadata": {},
   "source": [
    "### Repartition by column\n",
    "We can also repartition by columns.\n",
    "For example, let’s run the code below to repartition the data by column Country.\n",
    "This will create 200 partitions (**Spark by default create 200 partitions**).  However only three sharded files are generated:\n",
    "- One file stores data for CN country.\n",
    "- Another file stores data for AU country.\n",
    "- The other one is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ed39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(\"Country\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0e8c0",
   "metadata": {},
   "source": [
    "Similarly, if we can also partition the data by Date column:\n",
    "<pre>\n",
    "df = df.repartition(\"Date\")\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)\n",
    "</pre>\n",
    "If you look into the data, you may find the data is probably not partitioned properly as you would expect, for example, one partition file only includes data for both countries and different dates too.\n",
    "\n",
    "**This is because by default Spark use hash partitioning as partition function**. You can use range partitioning function or customize the partition functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ecb34",
   "metadata": {},
   "source": [
    "### Partition by multiple columns\n",
    "In real world, you would probably partition your data by multiple columns. To implement the multiple column partitioning strategy, we need to derive some new columns (year, month, date). Code below derives some new columns and then repartition the data frame with those columns.\n",
    "\n",
    "When you look into the saved files, you may find that all the new columns are also saved and the files still mix different sub partitions. To improve this, we need to match our write partition keys with repartition keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive some new columns (year, month, date)\n",
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\n",
    "\"Month\", month(\"Date\")).withColumn(\"Day\", dayofmonth(\"Date\"))\n",
    "# repartition the data frame with new columns\n",
    "df = df.repartition(\"Year\", \"Month\", \"Day\", \"Country\")\n",
    "df.show()\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.mode(\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9bb37ed0",
   "metadata": {},
   "source": [
    "### partitionBy\n",
    "When you look into the saved files, you may find that all the new columns are also saved and the files still mix different sub partitions. To improve this, we need to match our write partition keys with repartition keys.\n",
    "To match partition keys, we just need to change the last line to add a partitionBy function:\n",
    "\n",
    "When you open the generated files, you will also find that all the partitioning columns/keys are removed from the serialized data files.\n",
    "![image.png](attachment:image.png)\n",
    "In this way, the storage cost is also less. With partitioned data, we can also easily append data to new subfolders instead of operating on the complete data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive some new columns (year, month, date)\n",
    "df = df.withColumn(\"Year\", year(\"Date\")).withColumn(\n",
    "\"Month\", month(\"Date\")).withColumn(\"Day\", dayofmonth(\"Date\"))\n",
    "# repartition the data frame with new columns\n",
    "df = df.repartition(\"Year\", \"Month\", \"Day\", \"Country\")\n",
    "df.show()\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.partitionBy(\"Year\", \"Month\", \"Day\", \"Country\").mode(\n",
    "\"overwrite\").csv(\"data/example.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d18b5",
   "metadata": {},
   "source": [
    "### range repartition by day\n",
    "Note: Due to performance reasons this method uses sampling to estimate the ranges. Hence, the output may not be consistent, since sampling can return different values. The sample size can be controlled by the config spark.sql.execution.rangeExchange.sampleSizePerPartition.\n",
    "ref: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.repartitionByRange.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd91215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(\"Day\")\n",
    "df.show(5)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "print(3*'-', 'VERSUS repartitionByRange, observe the number of partitions ', 21*'-')\n",
    "\n",
    "df = df.repartitionByRange(\"Month\",\"Day\")\n",
    "df.show(5)\n",
    "print(df.rdd.getNumPartitions())\n",
    "print(3*'-', 'Observe the change in number of partitions ', 21*'-')\n",
    "df = df.repartitionByRange(\"Day\")\n",
    "df.show(5)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566bb5f",
   "metadata": {},
   "source": [
    "### Read partitioned Data after partitionBy\n",
    "\n",
    "Let’s read the data from the partitioned files with the these criteria:\n",
    "- Year= 2019\n",
    "- Month=2\n",
    "- Day=1\n",
    "- Country=CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/example.csv/Year=2019/Month=2/Day=1/Country=CN\")\n",
    "print('*'*60)\n",
    "print(f'Partitions in this dataframe {df.rdd.getNumPartitions()}')\n",
    "print('*'*60)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa48510",
   "metadata": {},
   "source": [
    "- Query all the data for the second month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/example.csv/Year=2019/Month=2\")\n",
    "print('*'*60)\n",
    "print(f'Partitions in this dataframe {df.rdd.getNumPartitions()}')\n",
    "print('*'*60)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3e69d",
   "metadata": {},
   "source": [
    "### Use wildcards for partition discovery\n",
    "We can use wildcards. Wildcards are supported for all file formats in partition discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"basePath\", \"data/example.csv/\").csv(\n",
    "\"data/example.csv/Year=*/Month=*/Day=*/Country=CN\")\n",
    "print('*'*60)\n",
    "print(f'Wildcard with Country CN: Partitions in this dataframe {df.rdd.getNumPartitions()}')\n",
    "print('*'*60)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7284c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"basePath\", \"data/example.csv/\").csv(\n",
    "\"data/example.csv/Year=*/Month=2/Day=*/Country=AU\")\n",
    "print('*'*60)\n",
    "print(f'Wildcard with Country AU and Month 2: Partitions in this dataframe {df.rdd.getNumPartitions()}')\n",
    "print('*'*60)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ffa39",
   "metadata": {},
   "source": [
    "## Print partition details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# funtion to print partition details  \n",
    "# print_partitions function will print out all the details about the RDD partitions\n",
    "# including the rows in each partition.\n",
    "#\n",
    "def print_partitions(df):\n",
    "    numPartitions = df.rdd.getNumPartitions()\n",
    "    print(\"Total partitions: {}\".format(numPartitions))\n",
    "    print(\"Partitioner: {}\".format(df.rdd.partitioner))\n",
    "    df.explain()\n",
    "    parts = df.rdd.glom().collect()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in parts:\n",
    "        print(\"Partition {}:\".format(i))\n",
    "        for r in p:\n",
    "            print(\"Row {}:{}\".format(j, r))\n",
    "            j = j+1\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3cab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate sample data\n",
    "countries = (\"CN\", \"AU\", \"US\")\n",
    "data = []\n",
    "for i in range(1, 13):\n",
    "    data.append({\"ID\": i, \"Country\": countries[i % 3],  \"Amount\": 10+i})\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "print_partitions(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a3b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repartition data\n",
    "# Let’s repartition the data to three partitions only by Country column.\n",
    "\n",
    "numPartitions = 3\n",
    "df = df.repartition(numPartitions, \"Country\")\n",
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0ff15",
   "metadata": {},
   "source": [
    "## Hashpartition\n",
    "You may expect that each partition includes data for each Country but that is not the case. \n",
    "Why? Because repartition function by default uses hash partitioning. For different country code,\n",
    "it may be allocated into the same partition number.\n",
    "\n",
    "We can verify this by using the following code to calculate the hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.rdd import portable_hash\n",
    "\n",
    "# define udf\n",
    "udf_portable_hash = udf(lambda str: portable_hash(str))\n",
    "\n",
    "df = df.withColumn(\"Hash#\", udf_portable_hash(df.Country))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da397259",
   "metadata": {},
   "source": [
    "## Allocate one partition for each key value\n",
    "For the above example, if we want to allocate one partition for each Country (CN, US, AU), what should we do?\n",
    "\n",
    "Well, the first thing we can try is to increase the partition number. In this way, the chance for allocating each different value to different partition is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "numPartitions = 5\n",
    "\n",
    "df = df.repartition(numPartitions, \"Country\")\n",
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a689f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_portable_hash = udf(lambda str: portable_hash(str))\n",
    "df = df.withColumn(\"Hash#\", udf_portable_hash(df.Country))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8032d",
   "metadata": {},
   "source": [
    "**Note***  the hashing algorithm generates the same hash code/number for the row with country US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "notice here we do not specify the number of partitions at repartition time \n",
    "it uses the default number for of partitions which is 200\n",
    "and you will notice this process will get very slow.\n",
    "commented out on purpose\n",
    "'''\n",
    "# df = df.repartition(\"Country\")\n",
    "# print_partitions(df)\n",
    "\n",
    "# udf_portable_hash = udf(lambda str: portable_hash(str))\n",
    "# df = df.withColumn(\"Hash#\", udf_portable_hash(df.Country))\n",
    "# df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e7cc3",
   "metadata": {},
   "source": [
    "## Custom hash partition using user defined partition column\n",
    "There is no direct way to apply user defined partitioner on PySpark, the short cut is to create a new column with a UDF, assigning each record with a partition ID based on the business logic. And use the new column for partitioning, that way the data gets spread evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918078c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.rdd import portable_hash\n",
    "\n",
    "# Populate sample data again\n",
    "countries = (\"CN\", \"AU\", \"US\")\n",
    "data = []\n",
    "for i in range(1, 13):\n",
    "    data.append({\"ID\": i, \"Country\": countries[i % 3],  \"Amount\": 10+i})\n",
    " \n",
    "df = spark.createDataFrame(data)\n",
    "# df.show()\n",
    "print(80*'-')\n",
    "\n",
    "# adding new column for partitioning\n",
    "countries = {\"CN\":100,\"AU\":200, \"US\":300}\n",
    "def country_partitioning(k):    return countries[k]\n",
    "udf_country_hash = F.udf(lambda str: country_partitioning(str))\n",
    "             \n",
    "numPartitions = 3\n",
    "df = df.withColumn(\"Hash#\", udf_country_hash(df['Country']))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "df.orderBy('Country').show()   \n",
    "df=df.repartition(3, \"Partition#\")\n",
    "print_partitions(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
