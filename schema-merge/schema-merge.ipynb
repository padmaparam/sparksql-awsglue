{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0bb7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5c0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scripts instantiates a SparkSession locally with 8 worker threads.\n",
    "'''\n",
    "appName = \"Parquet Schema Merge\"\n",
    "master = \"local[8]\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# Create Spark session\n",
    "conf = SparkConf().setMaster(master)\n",
    "spark = SparkSession.builder.config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "# INFO/WARN/DEBUG\n",
    "# https://kontext.tech/column/spark/457/tutorial-turn-off-info-logs-in-spark\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789b3e6",
   "metadata": {},
   "source": [
    "Code snippet simply create three dataframes from Python dictionary list. The schema for the data frame will be inferred automatically though the recommended approach is to specify the schema manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2880b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [{\"id\": 1, \"attr0\": \"Attr 0\"}, \n",
    "{\"id\": 2, \"attr0\": \"Attr 0\"}]\n",
    "df1 = spark.createDataFrame(data1)\n",
    "\n",
    "data2 = [{\"id\": 1, \"attr0\": \"Attr 0\", \"attr1\": \"Attr 1\"}, \n",
    "{\"id\": 2, \"attr0\": \"Attr 0\", \"attr1\": \"Attr 1\"}]\n",
    "df2 = spark.createDataFrame(data2)\n",
    "\n",
    "data3= [{\"id\": 1, \"attr1\": \"Attr 1\"}, \n",
    "{\"id\": 2, \"attr1\": \"Attr 1\"}]\n",
    "df3 = spark.createDataFrame(data3)\n",
    "\n",
    "df1.write.mode('overwrite').parquet('data/partition-date=2020-01-01')\n",
    "df2.write.mode('overwrite').parquet('data/partition-date=2020-01-02')\n",
    "df3.write.mode('overwrite').parquet('data/partition-date=2020-01-03')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02916b",
   "metadata": {},
   "source": [
    "### See info logs\n",
    "Make sure you set log level to INFO\n",
    "\n",
    "- Dataframe1\n",
    "<pre>\n",
    "22/02/09 06:50:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
    "{\n",
    "  \"type\" : \"struct\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"attr0\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"id\",\n",
    "    \"type\" : \"long\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  } ]\n",
    "}\n",
    "and corresponding Parquet message type:\n",
    "message spark_schema {\n",
    "  optional binary attr0 (UTF8);\n",
    "  optional int64 id;\n",
    "}\n",
    "</pre>\n",
    "\n",
    "- DataFrame2\n",
    "<pre>\n",
    "22/02/09 06:50:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
    "{\n",
    "  \"type\" : \"struct\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"attr0\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"attr1\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"id\",\n",
    "    \"type\" : \"long\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  } ]\n",
    "}\n",
    "and corresponding Parquet message type:\n",
    "message spark_schema {\n",
    "  optional binary attr0 (UTF8);\n",
    "  optional binary attr1 (UTF8);\n",
    "  optional int64 id;\n",
    "}\n",
    "</pre>\n",
    "\n",
    "- DataFrame3\n",
    "<pre>\n",
    "22/02/09 06:51:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
    "{\n",
    "  \"type\" : \"struct\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"attr1\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"id\",\n",
    "    \"type\" : \"long\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  } ]\n",
    "}\n",
    "and corresponding Parquet message type:\n",
    "message spark_schema {\n",
    "  optional binary attr1 (UTF8);\n",
    "  optional int64 id;\n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70537390",
   "metadata": {},
   "source": [
    "### Schema merge at time of Read\n",
    "Spark read with merge schema option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c936c52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+--------------+\n",
      "| attr0| id| attr1|partition-date|\n",
      "+------+---+------+--------------+\n",
      "|Attr 0|  1|Attr 1|    2020-01-02|\n",
      "|Attr 0|  2|Attr 1|    2020-01-02|\n",
      "|Attr 0|  1|  null|    2020-01-01|\n",
      "|Attr 0|  2|  null|    2020-01-01|\n",
      "|  null|  1|Attr 1|    2020-01-03|\n",
      "|  null|  2|Attr 1|    2020-01-03|\n",
      "+------+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"mergeSchema\",\"true\").parquet(\"data\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f3c66",
   "metadata": {},
   "source": [
    ">If we don't specify mergeSchema option, the new attributes will not be picked up.\n",
    "Without schema merge, the schema will be decided randomly based on on of the \n",
    "partition files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "563f3d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+\n",
      "| attr0| id|partition-date|\n",
      "+------+---+--------------+\n",
      "|Attr 0|  1|    2020-01-02|\n",
      "|Attr 0|  2|    2020-01-02|\n",
      "|Attr 0|  1|    2020-01-01|\n",
      "|Attr 0|  2|    2020-01-01|\n",
      "|  null|  1|    2020-01-03|\n",
      "|  null|  2|    2020-01-03|\n",
      "+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6f0a3",
   "metadata": {},
   "source": [
    "### Use Spark SQL\n",
    "- Alternatively, we can also use Spark SQL option to enable schema merge.\n",
    "- The result is same as using mergeSchema option. The advantage of using this option is that it is effective in the whole Spark session instead of specifying it in all read functions like spark.read.option(\"mergeSchema\",\"true\").parquet(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5896d7cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+--------------+\n",
      "| attr0| id| attr1|partition-date|\n",
      "+------+---+------+--------------+\n",
      "|Attr 0|  1|Attr 1|    2020-01-02|\n",
      "|Attr 0|  2|Attr 1|    2020-01-02|\n",
      "|Attr 0|  1|  null|    2020-01-01|\n",
      "|Attr 0|  2|  null|    2020-01-01|\n",
      "|  null|  1|Attr 1|    2020-01-03|\n",
      "|  null|  2|Attr 1|    2020-01-03|\n",
      "+------+---+------+--------------+\n",
      "\n",
      "root\n",
      " |-- attr0: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- attr1: string (nullable = true)\n",
      " |-- partition-date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\")\n",
    "df = spark.read.parquet(\"data\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45d3ae",
   "metadata": {},
   "source": [
    "### Schema merge at the time of  write/create\n",
    "- Schema merge at the time of write\n",
    "- Schema change with no data type conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "550ee882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.format(\"delta\").mode('overwrite').option(\"mergeSchema\", True).parquet('data/partition-date=2020-01-01')\n",
    "df2.write.format(\"delta\").mode('overwrite').option(\"mergeSchema\", True).parquet('data/partition-date=2020-01-02')\n",
    "df3.write.format(\"delta\").mode('overwrite').option(\"mergeSchema\", True).parquet('data/partition-date=2020-01-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "354ac34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+--------------+\n",
      "| attr0| id| attr1|partition-date|\n",
      "+------+---+------+--------------+\n",
      "|Attr 0|  1|Attr 1|    2020-01-02|\n",
      "|Attr 0|  2|Attr 1|    2020-01-02|\n",
      "|Attr 0|  1|  null|    2020-01-01|\n",
      "|Attr 0|  2|  null|    2020-01-01|\n",
      "|  null|  1|Attr 1|    2020-01-03|\n",
      "|  null|  2|Attr 1|    2020-01-03|\n",
      "+------+---+------+--------------+\n",
      "\n",
      "root\n",
      " |-- attr0: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- attr1: string (nullable = true)\n",
      " |-- partition-date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
