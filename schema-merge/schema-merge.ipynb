{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0bb7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5c0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scripts instantiates a SparkSession locally with 8 worker threads.\n",
    "'''\n",
    "appName = \"Parquet Schema Merge\"\n",
    "master = \"local[8]\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# Create Spark session\n",
    "conf = SparkConf().setMaster(master)\n",
    "spark = SparkSession.builder.config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "# INFO/WARN/DEBUG\n",
    "# https://kontext.tech/column/spark/457/tutorial-turn-off-info-logs-in-spark\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789b3e6",
   "metadata": {},
   "source": [
    "Code snippet simply create three dataframes from Python dictionary list. The schema for the data frame will be inferred automatically though the recommended approach is to specify the schema manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2880b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [{\"id\": 1, \"attr0\": \"Attr 0\"}, \n",
    "{\"id\": 2, \"attr0\": \"Attr 0\"}]\n",
    "df1 = spark.createDataFrame(data1)\n",
    "\n",
    "data2 = [{\"id\": 1, \"attr0\": \"Attr 0\", \"attr1\": \"Attr 1\"}, \n",
    "{\"id\": 2, \"attr0\": \"Attr 0\", \"attr1\": \"Attr 1\"}]\n",
    "df2 = spark.createDataFrame(data2)\n",
    "\n",
    "data3= [{\"id\": 1, \"attr1\": \"Attr 1\"}, \n",
    "{\"id\": 2, \"attr1\": \"Attr 1\"}]\n",
    "df3 = spark.createDataFrame(data3)\n",
    "\n",
    "df1.write.mode('overwrite').parquet('data/partition-date=2020-01-01')\n",
    "df2.write.mode('overwrite').parquet('data/partition-date=2020-01-02')\n",
    "df3.write.mode('overwrite').parquet('data/partition-date=2020-01-03')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02916b",
   "metadata": {},
   "source": [
    "### See info logs\n",
    "Make sure you set log level to INFO\n",
    "\n",
    "- Dataframe1\n",
    "<pre>\n",
    "22/02/09 06:50:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
    "{\n",
    "  \"type\" : \"struct\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"attr0\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"id\",\n",
    "    \"type\" : \"long\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  } ]\n",
    "}\n",
    "and corresponding Parquet message type:\n",
    "message spark_schema {\n",
    "  optional binary attr0 (UTF8);\n",
    "  optional int64 id;\n",
    "}\n",
    "</pre>\n",
    "\n",
    "- DataFrame2\n",
    "<pre>\n",
    "22/02/09 06:50:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
    "{\n",
    "  \"type\" : \"struct\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"attr0\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"attr1\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"id\",\n",
    "    \"type\" : \"long\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  } ]\n",
    "}\n",
    "and corresponding Parquet message type:\n",
    "message spark_schema {\n",
    "  optional binary attr0 (UTF8);\n",
    "  optional binary attr1 (UTF8);\n",
    "  optional int64 id;\n",
    "}\n",
    "</pre>\n",
    "\n",
    "- DataFrame3\n",
    "<pre>\n",
    "22/02/09 06:51:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
    "{\n",
    "  \"type\" : \"struct\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"attr1\",\n",
    "    \"type\" : \"string\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  }, {\n",
    "    \"name\" : \"id\",\n",
    "    \"type\" : \"long\",\n",
    "    \"nullable\" : true,\n",
    "    \"metadata\" : { }\n",
    "  } ]\n",
    "}\n",
    "and corresponding Parquet message type:\n",
    "message spark_schema {\n",
    "  optional binary attr1 (UTF8);\n",
    "  optional int64 id;\n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70537390",
   "metadata": {},
   "source": [
    "### Schema merge at time of Read\n",
    "Spark read with merge schema option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c936c52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+--------------+\n",
      "| attr0| id| attr1|partition-date|\n",
      "+------+---+------+--------------+\n",
      "|Attr 0|  1|Attr 1|    2020-01-02|\n",
      "|Attr 0|  2|Attr 1|    2020-01-02|\n",
      "|Attr 0|  1|  null|    2020-01-01|\n",
      "|Attr 0|  2|  null|    2020-01-01|\n",
      "|  null|  1|Attr 1|    2020-01-03|\n",
      "|  null|  2|Attr 1|    2020-01-03|\n",
      "+------+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"mergeSchema\",\"true\").parquet(\"data\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f3c66",
   "metadata": {},
   "source": [
    ">If we don't specify mergeSchema option, the new attributes will not be picked up.\n",
    "Without schema merge, the schema will be decided randomly based on on of the \n",
    "partition files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "563f3d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------------+\n",
      "| attr0| id|partition-date|\n",
      "+------+---+--------------+\n",
      "|Attr 0|  1|    2020-01-02|\n",
      "|Attr 0|  2|    2020-01-02|\n",
      "|Attr 0|  1|    2020-01-01|\n",
      "|Attr 0|  2|    2020-01-01|\n",
      "|  null|  1|    2020-01-03|\n",
      "|  null|  2|    2020-01-03|\n",
      "+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6f0a3",
   "metadata": {},
   "source": [
    "### Use Spark SQL\n",
    "- Alternatively, we can also use Spark SQL option to enable schema merge.\n",
    "- The result is same as using mergeSchema option. The advantage of using this option is that it is effective in the whole Spark session instead of specifying it in all read functions like spark.read.option(\"mergeSchema\",\"true\").parquet(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5896d7cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+--------------+\n",
      "| attr0| id| attr1|partition-date|\n",
      "+------+---+------+--------------+\n",
      "|Attr 0|  1|Attr 1|    2020-01-02|\n",
      "|Attr 0|  2|Attr 1|    2020-01-02|\n",
      "|Attr 0|  1|  null|    2020-01-01|\n",
      "|Attr 0|  2|  null|    2020-01-01|\n",
      "|  null|  1|Attr 1|    2020-01-03|\n",
      "|  null|  2|Attr 1|    2020-01-03|\n",
      "+------+---+------+--------------+\n",
      "\n",
      "root\n",
      " |-- attr0: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- attr1: string (nullable = true)\n",
      " |-- partition-date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"true\")\n",
    "df = spark.read.parquet(\"data\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f45d3ae",
   "metadata": {},
   "source": [
    "### Schema merge at the time of  write/create\n",
    "- Schema merge at the time of write\n",
    "- Schema change with no data type conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "550ee882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.format(\"delta\").mode('overwrite').option(\"mergeSchema\", True).parquet('data/partition-date=2020-01-01')\n",
    "df2.write.format(\"delta\").mode('overwrite').option(\"mergeSchema\", True).parquet('data/partition-date=2020-01-02')\n",
    "df3.write.format(\"delta\").mode('overwrite').option(\"mergeSchema\", True).parquet('data/partition-date=2020-01-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "354ac34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+--------------+\n",
      "| attr0| id| attr1|partition-date|\n",
      "+------+---+------+--------------+\n",
      "|Attr 0|  1|Attr 1|    2020-01-02|\n",
      "|Attr 0|  2|Attr 1|    2020-01-02|\n",
      "|Attr 0|  1|  null|    2020-01-01|\n",
      "|Attr 0|  2|  null|    2020-01-01|\n",
      "|  null|  1|Attr 1|    2020-01-03|\n",
      "|  null|  2|Attr 1|    2020-01-03|\n",
      "+------+---+------+--------------+\n",
      "\n",
      "root\n",
      " |-- attr0: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- attr1: string (nullable = true)\n",
      " |-- partition-date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b9c81",
   "metadata": {},
   "source": [
    "### Spark Merge Two DataFrames with Different Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1d18330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- dept: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [{\"name\": \"James\", \"dept\": \"Sales\", \"age\": 34}, \n",
    "         {\"name\": \"Michael\", \"dept\": \"Sales\", \"age\": 56},\n",
    "         {\"name\": \"Robert\", \"dept\": \"Sales\", \"age\": 34},\n",
    "         {\"name\": \"Maria\", \"dept\": \"Finance\", \"age\": 24}]\n",
    "df1 = spark.createDataFrame(data1)\n",
    "df1.printSchema()\n",
    "\n",
    "\n",
    "#val data2=Seq((\"James\",\"Sales\",\"NY\",9000),(\"Maria\",\"Finance\",\"CA\",9000),\n",
    "#              (\"Jen\",\"Finance\",\"NY\",7900),(\"Jeff\",\"Marketing\",\"CA\",8000))\n",
    "# data2.toDF(\"name\",\"dept\",\"state\",\"salary\")\n",
    "\n",
    "data2 = [{\"name\": \"James\", \"dept\": \"Sales\", \"state\": 'NY', \"salary\":9000}, \n",
    "         {\"name\": \"Maria\", \"dept\": \"Finance\", \"state\": 'CA', \"salary\":9000},\n",
    "         {\"name\": \"Jen\", \"dept\": \"Finance\", \"state\": 'NY', \"salary\":7900},\n",
    "         {\"name\": \"Jeff\", \"dept\": \"Marketing\", \"state\": 'CA', \"salary\":8000}]    \n",
    "\n",
    "\n",
    "df2 = spark.createDataFrame(data2)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d74a6",
   "metadata": {},
   "source": [
    "### dataframe union vs unionByName\n",
    "\n",
    "- union : this function resolves columns by position (not by name). That is the reason why when you union 2 dataframes the values may be swapped and one column from second dataframe is missing.\n",
    "\n",
    "- unionByName : You should use unionByName, but this functions requires both dataframe to have the same structure.\n",
    "See below, simple code to harmonize the structure of your dataframes and then do the union(ByName)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bd7794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# harmonize dataframes\n",
    "def add_missing_columns(df: DataFrame, ref_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add missing columns from ref_df to df\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): dataframe with missing columns\n",
    "        ref_df (DataFrame): referential dataframe\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: df with additionnal columns from ref_df\n",
    "    \"\"\"\n",
    "    for col in ref_df.schema:\n",
    "        if col.name not in df.columns:\n",
    "            df = df.withColumn(col.name, F.lit(None).cast(col.dataType))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc2f21",
   "metadata": {},
   "source": [
    "### Adding missing columns to dataframe1, df1 and dataframe2, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb295261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----+---------+-------+------+-----+\n",
      "| age|     dept|   name|salary|state|\n",
      "+----+---------+-------+------+-----+\n",
      "|  34|    Sales|  James|  null| null|\n",
      "|  56|    Sales|Michael|  null| null|\n",
      "|  34|    Sales| Robert|  null| null|\n",
      "|  24|  Finance|  Maria|  null| null|\n",
      "|null|    Sales|  James|  9000|   NY|\n",
      "|null|  Finance|  Maria|  9000|   CA|\n",
      "|null|  Finance|    Jen|  7900|   NY|\n",
      "|null|Marketing|   Jeff|  8000|   CA|\n",
      "+----+---------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = add_missing_columns(df1, df2)\n",
    "df2 = add_missing_columns(df2, df1)\n",
    "\n",
    "df_result = df1.unionByName(df2)\n",
    "df_result.printSchema()\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae4bab",
   "metadata": {},
   "source": [
    "### PySpark Merge Two DataFrames with Different Columns\n",
    "Harmonize dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97614bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "#Create DataFrame df1 with columns name,dept & age\n",
    "data = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56), \\\n",
    "    (\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\n",
    "columns= [\"name\",\"dept\",\"age\"]\n",
    "df1 = spark.createDataFrame(data = data, schema = columns)\n",
    "df1.printSchema()\n",
    "\n",
    "#Create DataFrame df1 with columns name,dep,state & salary\n",
    "data2=[(\"James\",\"Sales\",\"NY\",9000),(\"Maria\",\"Finance\",\"CA\",9000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",7900),(\"Jeff\",\"Marketing\",\"CA\",8000)]\n",
    "columns2= [\"name\",\"dept\",\"state\",\"salary\"]\n",
    "df2 = spark.createDataFrame(data = data2, schema = columns2)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d83fd5",
   "metadata": {},
   "source": [
    "Now **add missing columns** ‘state‘ and ‘salary‘ to df1 and ‘age‘ to df2 with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78eaeb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- state: null (nullable = true)\n",
      " |-- salary: null (nullable = true)\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: null (nullable = true)\n",
      "\n",
      "+-------+---------+----+-----+------+\n",
      "|   name|     dept| age|state|salary|\n",
      "+-------+---------+----+-----+------+\n",
      "|  James|    Sales|  34| null|  null|\n",
      "|Michael|    Sales|  56| null|  null|\n",
      "| Robert|    Sales|  30| null|  null|\n",
      "|  Maria|  Finance|  24| null|  null|\n",
      "|  James|    Sales|null|   NY|  9000|\n",
      "|  Maria|  Finance|null|   CA|  9000|\n",
      "|    Jen|  Finance|null|   NY|  7900|\n",
      "|   Jeff|Marketing|null|   CA|  8000|\n",
      "+-------+---------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add missing columns 'state' & 'salary' to df1\n",
    "from pyspark.sql.functions import lit\n",
    "for column in [column for column in df2.columns if column not in df1.columns]:\n",
    "    df1 = df1.withColumn(column, lit(None))\n",
    "\n",
    "df1.printSchema()    \n",
    "    \n",
    "#Add missing column 'age' to df2\n",
    "for column in [column for column in df1.columns if column not in df2.columns]:\n",
    "    df2 = df2.withColumn(column, lit(None))\n",
    "df2.printSchema()  \n",
    "    \n",
    "#Finally join two dataframe's df1 & df2 by name\n",
    "merged_df=df1.unionByName(df2)\n",
    "merged_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
