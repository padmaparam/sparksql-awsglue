{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2742d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc74226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark_aw_glue>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "# ref: https://towardsai.net/p/programming/pyspark-aws-s3-read-write-operations\n",
    "#spark configuration\n",
    "conf = SparkConf().set('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true'). \\\n",
    " setAppName('pyspark_aw_glue').setMaster('local[*]')\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "# read aws credentials\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(r'C:\\Users\\padma\\.aws\\credentials'))\n",
    "\n",
    "accessKeyId= config['default']['AWS_ACCESS_KEY_ID']\n",
    "secretAccessKey= config['default']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', accessKeyId)\n",
    "hadoopConf.set('fs.s3a.secret.key', secretAccessKey)\n",
    "hadoopConf.set('fs.s3a.endpoint', 's3.amazonaws.com')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "print(sc)\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ea7ff",
   "metadata": {},
   "source": [
    "### Create glue catalog via boto3\n",
    "https://stackoverflow.com/questions/58329935/how-to-create-a-data-catalog-in-amazon-glue-externally]\n",
    "\n",
    "#### Create sample_db database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0809656",
   "metadata": {},
   "outputs": [
    {
     "ename": "AlreadyExistsException",
     "evalue": "An error occurred (AlreadyExistsException) when calling the CreateDatabase operation: Database already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAlreadyExistsException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-642d75c8b6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m response = client.create_database(\n\u001b[0m\u001b[0;32m      6\u001b[0m     DatabaseInput={\n\u001b[0;32m      7\u001b[0m         \u001b[1;34m'Name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'sample_db'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[0;32m    356\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padma\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAlreadyExistsException\u001b[0m: An error occurred (AlreadyExistsException) when calling the CreateDatabase operation: Database already exists."
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('glue')\n",
    "response = client.create_database(\n",
    "    DatabaseInput={\n",
    "        'Name': 'sample_db',  # Required\n",
    "        'Description': 'Database created with boto3 API',\n",
    "        'Parameters': {\n",
    "            'my_param_1': 'my_param_value_1'\n",
    "        },\n",
    "    }\n",
    ")\n",
    "pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26496914",
   "metadata": {},
   "source": [
    "#### Delete sample_db database in glue catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6937389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import boto3\n",
    "\n",
    "#client = boto3.client('glue')\n",
    "#response = client.delete_database(\n",
    "#    #CatalogId='string',\n",
    "#    Name='sample_db'\n",
    "#)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ca9a6",
   "metadata": {},
   "source": [
    "#### Reload the aws_glue package wiithout restarting the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5918ccee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'glue_helper.aws_glue' from 'C:\\\\Users\\\\padma\\\\github\\\\sparksql-awsglue\\\\aws-glue\\\\glue_helper\\\\aws_glue.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from glue_helper import aws_glue\n",
    "importlib.reload(aws_glue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e4197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glue_helper import aws_glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8873da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws_glue.create_table_helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3761d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this evety time you create the table or add a partition, \n",
    "#spark.sql('MSCK REPAIR TABLE sample_db.airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d2d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLIGHTS airlines date flight_number \n"
     ]
    }
   ],
   "source": [
    "aws_glue.get_current_schema_partition('sample_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried to fix table creates via aws_glue.create_table_helper(), but did not work\n",
    "#response = aws_glue.apply_msck_repair('sample_db', 'airlines')\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268eedd",
   "metadata": {},
   "source": [
    "#### Getting  a small data set and doing some experiments with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82764b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------+-----------+---------+---------------+-------+-------------+--------+--------+--------+\n",
      "|airlines|flight_number|origin|destination|departure|departure_delay|arrival|arrival_delay|air_time|distance|    date|\n",
      "+--------+-------------+------+-----------+---------+---------------+-------+-------------+--------+--------+--------+\n",
      "|   19690|            1|   LAX|        HNL|     0834|          -6.00|   1204|        44.00|  360.00| 2556.00|20140401|\n",
      "|   19690|            2|   HNL|        LAX|     1342|          -3.00|   2151|       -19.00|  279.00| 2556.00|20140401|\n",
      "|   19690|            3|   LAX|        HNL|     0952|          -8.00|   1251|        21.00|  339.00| 2556.00|20140401|\n",
      "|   19690|            4|   HNL|        LAX|     2215|           0.00|   0615|       -20.00|  273.00| 2556.00|20140401|\n",
      "|   19690|            7|   LAS|        HNL|     0848|         -12.00|   1219|         4.00|  376.00| 2762.00|20140401|\n",
      "|   19690|            8|   HNL|        LAS|     2226|          -9.00|   0646|       -29.00|  296.00| 2762.00|20140401|\n",
      "|   19690|           17|   LAS|        HNL|     0147|          -8.00|   0515|        15.00|  377.00| 2762.00|20140401|\n",
      "|   19690|           18|   HNL|        LAS|     1503|          -2.00|   2327|       -23.00|  299.00| 2762.00|20140401|\n",
      "|   19690|           19|   SMF|        HNL|     0904|         -11.00|   1205|        15.00|  346.00| 2462.00|20140401|\n",
      "|   19690|           20|   HNL|        SMF|     1415|          15.00|   2225|        10.00|  276.00| 2462.00|20140401|\n",
      "+--------+-------------+------+-----------+---------+---------------+-------+-------------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsPath = \"datasets/flights.csv\"\n",
    "flights = spark.read\\\n",
    "                .format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .load(flightsPath)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "flights= flights.withColumn(\"date1\", F.expr(\"replace(date, '-', '')\"))\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "flights = flights.withColumn(\"date1\",flights[\"date1\"].cast(IntegerType()))\n",
    "flights.select(\"date1\").dtypes\n",
    "\n",
    "flights_19690_20304 = flights.filter(flights['airlines'].isin([\"19690\", \"20304\"])) \n",
    "flights_19690_20304 = flights_19690_20304.filter(flights_19690_20304['date1'].isin([\"20140401\", \"20140402\"]))\n",
    "flights_19690_20304 = flights_19690_20304.drop(\"date\")\n",
    "flights_19690_20304 = flights_19690_20304.withColumnRenamed(\"date1\",\"date\") \n",
    "#flights_19690_20304.count()\n",
    "#flights_19690_20304.show()\n",
    "flights_sml = flights_19690_20304.limit(10)\n",
    "# small data set\n",
    "flights_sml.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a70bb8",
   "metadata": {},
   "source": [
    "#### Version and location of python exe  and othes used in this jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "\n",
    "#####################\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "\n",
    "# pyspark jar location i think is\n",
    "# C:\\Users\\padma\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\\jars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3c1d7",
   "metadata": {},
   "source": [
    "#### Create parquet to file in local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_sml.count()\n",
    "flights_sml.write.mode(\"overwrite\").partitionBy('airlines', 'date', 'flight_number').parquet(\"output/flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee8cc6",
   "metadata": {},
   "source": [
    "#### Create parquet with partitons file in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283a8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing parquet file to S3, without partitions\n",
    "flights_sml.write.mode(\"overwrite\").parquet(\"s3a://pp-database/tables/flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1a4c96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#writing parquet file to s3 with partitions\n",
    "aws_glue.write_parquet_to_s3(flights_sml, 's3a://pp-database/tables/flights',\n",
    "                            ['airlines', 'date', 'flight_number'])\n",
    "\n",
    "# create table in glue catalog\n",
    "columns_types= {'origin': 'string', 'destination': 'string', \\\n",
    "                'departure': 'string', 'departure_delay': 'string',\\\n",
    "                'arrival': 'string', 'arrival_delay': 'string',\\\n",
    "                'air_time': 'string', 'distance': 'string' }\n",
    "\n",
    "partitions_types = { 'airlines': 'string',\\\n",
    "                    'date': 'int',\\\n",
    "                    'flight_number': 'string' }\n",
    "\n",
    "aws_glue.create_glue_table('sample_db', 'flights', 's3://pp-database/tables/flights', columns_types, partitions_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fba69",
   "metadata": {},
   "source": [
    "#### Read back the recently created parquet file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2766a6f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-81d2a5aba27e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mflights_read_from_s3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's3a://pp-database/tables/flights'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mflights_read_from_s3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mflights_read_from_s3_filter_by_airline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's3a://pp-database/tables/flights/airlines=19690'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mflights_read_from_s3_filter_by_airline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1303\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m~\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1034\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padma\\appdata\\local\\programs\\python\\python38\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "flights_read_from_s3=spark.read.parquet('s3a://pp-database/tables/flights',header=True,inferSchema=True)\n",
    "flights_read_from_s3.show(5)\n",
    "\n",
    "flights_read_from_s3_filter_by_airline=spark.read.parquet('s3a://pp-database/tables/flights/airlines=19690',header=True,inferSchema=True)\n",
    "flights_read_from_s3_filter_by_airline.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad7fed",
   "metadata": {},
   "source": [
    "# Notes \n",
    "##### discard below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse aws credentials\n",
    "# import configparser\n",
    "\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read_file(open(r'C:\\Users\\padma\\.aws\\credentials'))\n",
    "\n",
    "#os.environ[\"AWS_ACCESS_KEY_ID\"]= config['default']['AWS_ACCESS_KEY_ID']\n",
    "#os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['default']['AWS_SECRET_ACCESS_KEY']\n",
    "#os.environ[\"SPARK_HOME\"]=r\"C:\\Users\\padma\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\"\n",
    "#os.environ[\"HADOOP_HOME\"]=r\"C:\\Users\\padma\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\\hadoop\"\n",
    "\n",
    "#import sys\n",
    "#sys.path.append(r\"C:\\Users\\padma\\spark\\spark-sql\\tools\\spark-3.1.2-bin-hadoop2.7\\hadoopbin\")\n",
    "\n",
    "#.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.4\") \\\n",
    "#.config(\"spark.jars.packages\", \"com.amazonaws:aws-java-sdk-pom:1.7.4.2\") \\\n",
    "#.config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "#.config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "\n",
    "# '''\n",
    "# fs.s3a.access.key your access key\n",
    "# fs.s3a.secret.key your secret key\n",
    "# (\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "# .config(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")\\\n",
    "# '''\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .config(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\\\n",
    "#     .config(\"com.amazonaws.services.s3a.enableV4\", \"true\")\\\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "#     .config(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "#     .config(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "#     .appName(\"Analyzing airline data\") \\\n",
    "#     .getOrCreate()\n",
    "# '''\n",
    "# spark = SparkSession \\\n",
    "# .builder \\\n",
    "# .config(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")\\\n",
    "# .config(\"com.amazonaws.services.s3a.enableV4\", \"true\")\\\n",
    "# .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "# .config(\"fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "# .config(\"fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "# .getOrCreate()\n",
    "# '''\n",
    "\n",
    "# sc = spark.sparkContext\n",
    "# print(sc)\n",
    "# sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e76b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
