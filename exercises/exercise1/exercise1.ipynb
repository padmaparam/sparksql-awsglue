{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f7f4ae",
   "metadata": {},
   "source": [
    "## Following is to load spark module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b7f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317300b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Analyze sparkpost data\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "\n",
    "appName = \"Analyze sparkpost data.\"\n",
    "# #local[*] --> Run Spark locally with as many worker threads as logical cores on your machine.\n",
    "master = \"local[*]\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# Create Spark session\n",
    "conf = SparkConf().setMaster(master).setAppName(appName)\n",
    "spark = SparkSession.builder.config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "# INFO/WARN/DEBUG\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad423bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_path = \"data/customers/customers.json\"\n",
    "#parquet format. Partition by date,dt, data/events/dt=2019-07-02\n",
    "events_path = \"data/events\"\n",
    "mailbox_providers_path = \"data/mailbox-providers/providers.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4c08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read events\n",
    "events_df=spark.read.parquet(events_path,header=True,inferSchema=True)\n",
    "\n",
    "# events_df.count()\n",
    "# events_df.show(9)\n",
    "import pyspark.sql.functions as F\n",
    "# adds name of the parquet file the row belongs to\n",
    "events_df=spark.read.parquet(events_path,header=True,inferSchema=True).withColumn('fileName', F.input_file_name())\n",
    "df_file_names= events_df.select('fileName').distinct()\n",
    "df_file_names.show()\n",
    "# file count \n",
    "file_count= events_df.select(F.countDistinct("fileName")).collect()[0][0]\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7718c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------+---------+----------+\n",
      "|customer_id|      rcpt_to|routing_domain|     type|        dt|\n",
      "+-----------+-------------+--------------+---------+----------+\n",
      "|          2|WPOb93It7ZRcO|     gmail.com|injection|2019-07-02|\n",
      "|          2|k2pHPOC4246Av|     gmail.com|injection|2019-07-02|\n",
      "|          2|4fi5kx/D09GJh|     gmail.com|injection|2019-07-02|\n",
      "|          2|VcPhQkr1Sbd7U|     gmail.com|injection|2019-07-02|\n",
      "|          2|/hLZtJLqq+uoD|     gmail.com|injection|2019-07-02|\n",
      "|          2|GNW4azf//ywuf|     gmail.com|injection|2019-07-02|\n",
      "|          2|TmeYaV52RQ1Ox|     yahoo.com|injection|2019-07-02|\n",
      "+-----------+-------------+--------------+---------+----------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9976987"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# events_df.printSchema()\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "events_df.createOrReplaceTempView(\"events\")\n",
    "\n",
    "# sql_events_df = spark.sql(\"SELECT distinct dt FROM events\")\n",
    "# sql_events_df.show()\n",
    "\n",
    "sql_injection_events_df = spark.sql(\"SELECT * FROM events where type = 'injection'\")\n",
    "sql_injection_events_df.show(7)\n",
    "sql_injection_events_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbacb7",
   "metadata": {},
   "source": [
    "## Read Customer json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4fdef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df = spark.read.json(customers_path)\n",
    "customers_df.printSchema()\n",
    "from pyspark.sql.functions import col\n",
    "customers_df = customers_df.filter(col(\"is_active\") == \"true\")\n",
    "# customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f2c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "customers_df.createOrReplaceTempView(\"active_customers\")\n",
    "sql_active_customers_df = spark.sql(\"SELECT  customer_id, is_active FROM active_customers\")\n",
    "# sql_active_customers_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0709ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- domains: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- group: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------+\n",
      "|             domains|        group|\n",
      "+--------------------+-------------+\n",
      "|[gmail.com, googl...|        gmail|\n",
      "|[verizon.net, aol...|verizon_media|\n",
      "|[yahoo.co.uk, yah...|     yahoo_uk|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mailbox_providers_df = spark.read.json(mailbox_providers_path)\n",
    "mailbox_providers_df.printSchema()\n",
    "mailbox_providers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a831904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|        domain|        group|\n",
      "+--------------+-------------+\n",
      "|     gmail.com|        gmail|\n",
      "|googlemail.com|        gmail|\n",
      "|   verizon.net|verizon_media|\n",
      "|       aol.com|verizon_media|\n",
      "|       aim.com|verizon_media|\n",
      "|     yahoo.com|verizon_media|\n",
      "|   yahoo.co.uk|     yahoo_uk|\n",
      "|      yahoo.fr|     yahoo_uk|\n",
      "|      yahoo.es|     yahoo_uk|\n",
      "|      yahoo.de|     yahoo_uk|\n",
      "|      yahoo.it|     yahoo_uk|\n",
      "|      yahoo.gr|     yahoo_uk|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/41027315/pyspark-split-multiple-array-columns-into-rows\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_mailbox_providers_df = mailbox_providers_df.withColumn('domains', explode('domains')).\\\n",
    "withColumnRenamed('domains', 'domain')\n",
    "exploded_mailbox_providers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3629579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|     provider|     domain|\n",
      "+-------------+-----------+\n",
      "|verizon_media|verizon.net|\n",
      "|verizon_media|    aol.com|\n",
      "|verizon_media|    aim.com|\n",
      "|verizon_media|  yahoo.com|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group1 = ['verizon_media']\n",
    "group2 = ['verizon_media', 'gmail']\n",
    "group3 = ['verizon_media', 'gmail', 'yahoo_uk']\n",
    "\n",
    "group_filter = group1\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "import pyspark.sql.functions as F\n",
    "exploded_mailbox_providers_df = exploded_mailbox_providers_df.filter(F.col(\"group\").isin(group_filter))\n",
    "# exploded_mailbox_providers_df.show()\n",
    "exploded_mailbox_providers_df.createOrReplaceTempView(\"mailbox_providers\")\n",
    "sql_mailbox_providers_df = spark.sql(\"SELECT group as provider, domain FROM mailbox_providers \")\n",
    "sql_mailbox_providers_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a851b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group1 = [\"'verizon_media'\"]\n",
    "# group2 = [\"'verizon_media'\", \"'gmail'\"]\n",
    "# group3 = [\"'verizon_media'\", \"'gmail'\", \"'yahoo_uk'\"]\n",
    "\n",
    "# group = group1\n",
    "\n",
    "# mailbox_providers_query = f\"SELECT group, domain FROM mailbox_providers where group in ({', '.join(group1)}) \"\n",
    "# print(mailbox_providers_query)\n",
    "# sql_mailbox_providers_df = spark.sql(mailbox_providers_query)\n",
    "# sql_mailbox_providers_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b203584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default value for spark.sql.autoBroadcastJoinThreshold: 10485760b\n"
     ]
    }
   ],
   "source": [
    "# 10 mb is the default\n",
    "print(f'default value for spark.sql.autoBroadcastJoinThreshold: {spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f3a72",
   "metadata": {},
   "source": [
    "## Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d24b0e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Exclude Threshold Enabled True \n",
      "Exclude Injection Event Counts Below: 100000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# variables and controls\n",
    "exclude_threshold = (True, 100000)\n",
    "window_function = False    \n",
    "type= \"'injection'\"\n",
    "\n",
    "print(80*'-')\n",
    "print(f'Exclude Threshold Enabled {exclude_threshold[0]} \\nExclude Injection Event Counts Below: {exclude_threshold[1]}')\n",
    "print(80*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2681cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SELECT ac.customer_id Customer, e.dt EventDate, mb.group Provider, count(type) InjectionEvents FROM events e INNER JOIN active_customers ac ON e.customer_id = ac.customer_id INNER JOIN mailbox_providers mb ON e.routing_domain = mb.domain WHERE e.type='injection' GROUP BY Customer, EventDate, Provider  HAVING InjectionEvents > 100000  ORDER BY  Customer, EventDate, Provider\n"
     ]
    }
   ],
   "source": [
    "#  /*+ MAPJOIN(mailbox_providers)*/ /*+ MAPJOIN(active_customers) */ \n",
    "#  /*+ BROADCAST(mailbox_providers)*/ /*+ BROADCAST(active_customers) */\n",
    "sql_query = ''\n",
    "sql_query_window_start = 'SELECT Customer, EventDate, Provider, InjectionEvents, ' \\\n",
    "'InjectionEvents *100/Total as PercentOfInjectionTrafficByProvider FROM ( '\\\n",
    "'SELECT Customer, EventDate, Provider, InjectionEvents, ' \\\n",
    "'SUM(InjectionEvents) OVER (PARTITION BY CUSTOMER, EventDate) as Total FROM ( ' \n",
    "\n",
    "sql_query = 'SELECT ac.customer_id Customer, e.dt EventDate, mb.group Provider, count(type) InjectionEvents ' \\\n",
    "'FROM events e INNER JOIN active_customers ac ON e.customer_id = ac.customer_id '\\\n",
    "f'INNER JOIN mailbox_providers mb ON e.routing_domain = mb.domain WHERE e.type={type} '\\\n",
    "'GROUP BY Customer, EventDate, Provider '\n",
    "\n",
    "if exclude_threshold[0]:\n",
    "    sql_query = f\"{sql_query} HAVING InjectionEvents > {exclude_threshold[1]}\"\n",
    "\n",
    "sql_query = f' {sql_query}  ORDER BY  Customer, EventDate, Provider'   \n",
    "\n",
    "sql_query_window_end = ' ) ORDER BY  Customer, EventDate, Provider )'\n",
    "\n",
    "if window_function:\n",
    "    sql_query =  f' {sql_query_window_start} {sql_query} {sql_query_window_end} '\n",
    "\n",
    "print(sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43d4002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------+---------------+\n",
      "|Customer| EventDate|     Provider|InjectionEvents|\n",
      "+--------+----------+-------------+---------------+\n",
      "|       2|2019-07-01|verizon_media|         107238|\n",
      "|       2|2019-07-03|verizon_media|         131554|\n",
      "|       2|2019-07-04|verizon_media|         104696|\n",
      "+--------+----------+-------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_df = spark.sql(sql_query)\n",
    "sql_df.show(25)\n",
    "sql_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c7796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(8) Project [Customer#373L, EventDate#374, Provider#375, InjectionEvents#376L, (cast((InjectionEvents#376L * 100) as double) / cast(Total#377L as double)) AS PercentOfInjectionTrafficByProvider#378]\n",
      "+- *(8) Sort [Customer#373L ASC NULLS FIRST, EventDate#374 ASC NULLS FIRST, Provider#375 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Customer#373L ASC NULLS FIRST, EventDate#374 ASC NULLS FIRST, Provider#375 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#646]\n",
      "      +- Window [sum(InjectionEvents#376L) windowspecdefinition(CUSTOMER#373L, EventDate#374, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS Total#377L], [CUSTOMER#373L, EventDate#374]\n",
      "         +- *(7) Sort [CUSTOMER#373L ASC NULLS FIRST, EventDate#374 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(CUSTOMER#373L, EventDate#374, 200), ENSURE_REQUIREMENTS, [id=#641]\n",
      "               +- *(6) Sort [Customer#373L ASC NULLS FIRST, EventDate#374 ASC NULLS FIRST, Provider#375 ASC NULLS FIRST], true, 0\n",
      "                  +- Exchange rangepartitioning(Customer#373L ASC NULLS FIRST, EventDate#374 ASC NULLS FIRST, Provider#375 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#637]\n",
      "                     +- *(5) HashAggregate(keys=[customer_id#53L, dt#4, group#67], functions=[count(type#3)])\n",
      "                        +- Exchange hashpartitioning(customer_id#53L, dt#4, group#67, 200), ENSURE_REQUIREMENTS, [id=#633]\n",
      "                           +- *(4) HashAggregate(keys=[customer_id#53L, dt#4, group#67], functions=[partial_count(type#3)])\n",
      "                              +- *(4) Project [type#3, dt#4, customer_id#53L, group#67]\n",
      "                                 +- *(4) BroadcastHashJoin [routing_domain#2], [domain#85], Inner, BuildRight, false\n",
      "                                    :- *(4) Project [routing_domain#2, type#3, dt#4, customer_id#53L]\n",
      "                                    :  +- *(4) BroadcastHashJoin [cast(customer_id#0 as bigint)], [customer_id#53L], Inner, BuildRight, false\n",
      "                                    :     :- *(4) Filter (((isnotnull(type#3) AND (type#3 = injection)) AND isnotnull(customer_id#0)) AND isnotnull(routing_domain#2))\n",
      "                                    :     :  +- *(4) ColumnarToRow\n",
      "                                    :     :     +- FileScan parquet [customer_id#0,routing_domain#2,type#3,dt#4] Batched: true, DataFilters: [isnotnull(type#3), (type#3 = injection), isnotnull(customer_id#0), isnotnull(routing_domain#2)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/padma/sparkpost/DataEngTakeHome/data/events], PartitionFilters: [], PushedFilters: [IsNotNull(type), EqualTo(type,injection), IsNotNull(customer_id), IsNotNull(routing_domain)], ReadSchema: struct<customer_id:int,routing_domain:string,type:string>\n",
      "                                    :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#615]\n",
      "                                    :        +- *(1) Project [customer_id#53L]\n",
      "                                    :           +- *(1) Filter ((isnotnull(is_active#54) AND (is_active#54 = true)) AND isnotnull(customer_id#53L))\n",
      "                                    :              +- FileScan json [customer_id#53L,is_active#54] Batched: false, DataFilters: [isnotnull(is_active#54), (is_active#54 = true), isnotnull(customer_id#53L)], Format: JSON, Location: InMemoryFileIndex[file:/C:/Users/padma/sparkpost/DataEngTakeHome/data/customers/customers.json], PartitionFilters: [], PushedFilters: [IsNotNull(is_active), EqualTo(is_active,true), IsNotNull(customer_id)], ReadSchema: struct<customer_id:bigint,is_active:boolean>\n",
      "                                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#627]\n",
      "                                       +- *(3) Project [domains#82 AS domain#85, group#67]\n",
      "                                          +- *(3) Filter isnotnull(domains#82)\n",
      "                                             +- Generate explode(domains#66), [group#67], false, [domains#82]\n",
      "                                                +- *(2) Filter ((group#67 IN (verizon_media,gmail,yahoo_uk) AND (size(domains#66, true) > 0)) AND isnotnull(domains#66))\n",
      "                                                   +- FileScan json [domains#66,group#67] Batched: false, DataFilters: [group#67 IN (verizon_media,gmail,yahoo_uk), (size(domains#66, true) > 0), isnotnull(domains#66)], Format: JSON, Location: InMemoryFileIndex[file:/C:/Users/padma/sparkpost/DataEngTakeHome/data/mailbox-providers/providers..., PartitionFilters: [], PushedFilters: [In(group, [verizon_media,gmail,yahoo_uk]), IsNotNull(domains)], ReadSchema: struct<domains:array<string>,group:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    " sql_query\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8500239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(sql_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcd28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = sql_df.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64ae4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df.write.partitionBy(\"EventDate\").mode(\"overwrite\").parquet(\"output/injection_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66835305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
